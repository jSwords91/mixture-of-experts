# Mixture of Experts from scratch

A PyTorch implementation of a Mixture of Experts (MoE) model.

Uses a basic expert which is easily extended, and a a top-k router with auxillary loss.

# To Do

Weight calculations for inference v full model
Capacity
GPT with MoE